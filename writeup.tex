%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%This is a science homework template. Modify the preamble to suit your needs. 
%The junk text is   there for you to immediately see how the headers/footers look at first 
%typesetting.


\documentclass[12pt]{article}

%AMS-TeX packages
\usepackage{amssymb,amsmath,amsthm} 
%geometry (sets margin) and other useful packages
\usepackage[margin=1.25in]{geometry}
\usepackage{graphicx,placeins}


%
%Redefining sections as problems
%
\makeatletter
\newenvironment{problem}{\@startsection
       {section}
       {1}
       {-.2em}
       {-3.5ex plus -1ex minus -.2ex}
       {2.3ex plus .2ex}
       {\pagebreak[3]%forces pagebreak when space is small; use \eject for better results
       \large\bf\noindent{Problem }
       }
       }
       {%\vspace{1ex}\begin{center} \rule{0.3\linewidth}{.3pt}\end{center}}
       \begin{center}\large\bf \ldots\ldots\ldots\end{center}}
\makeatother


%
%Fancy-header package to modify header/page numbering 
%
\usepackage{fancyhdr}
\pagestyle{fancy}
%\addtolength{\headwidth}{\marginparsep} %these change header-rule width
%\addtolength{\headwidth}{\marginparwidth}
\lhead{Problem \thesection}
\chead{} 
\rhead{\thepage} 
\lfoot{\small\scshape Machine Learning in Complex Domains} 
\cfoot{} 
\rfoot{\footnotesize PS \#3} 
\renewcommand{\headrulewidth}{.3pt} 
\renewcommand{\footrulewidth}{.3pt}
\setlength\voffset{-0.25in}
\setlength\textheight{648pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%
%Contents of problem set
%    
\begin{document}

\title{MLCD 3: Posterior Inference}
\author{Mike Smith and Elan Hourticolon-Retzler}

\maketitle

\thispagestyle{empty}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{2 Variational Inference - Mean-Field and Structured Mean-Field}
\noindent {\bf(2.1.a) Derivation of mean field given our network...}\\
For mean field, we are assuming an approximate distribution $Q = \prod_i Q(x_i)$, that is a product of marginals, to approximate our actual distribution $P$.  We also assume that $\forall i, \sum\limits_{x_i} Q(x_i) = 1$, that each marginal $Q(x_i)$ is a valid distribution in itself.\\
We want to optimize the energy functional, or minimize $\mathrm{KL}(q\|p)$.  The energy functional is as follows:
\begin{align*}
F &= \mathrm{E}_Q \ln P + \mathrm{H}_Q(X) \\
&= \mathrm{E}_Q \ln P + \mathrm{E}_Q \ln Q \\
&= \sum_{\phi \in \Phi} \mathrm{E}_Q [\ln \phi] + \mathrm{E}_Q \ln Q
\end{align*}
The first term is equal to the following, since $Q$ is a product of marginals:
\begin{equation*}
\mathrm{E}_Q \ln P = \mathrm{E}_{U_\phi \sim Q} [\ln \phi] = \sum_{u_\phi} Q(u_\phi) \ln \phi(u_\phi) = \sum_{u_\phi} \left( \prod_{X_i \in U_\phi } Q(x_i) \right) \ln \phi(u_\phi)
\end{equation*}
The second term, the entropy term, is equal to:
\begin{equation*}
\mathrm{H}_Q(X) = \mathrm{E}_Q \ln Q = \sum_i \mathrm{H}_Q(X_i)
\end{equation*}
We want to optimize the energy functional, subject to our given constraints, so we write a Lagrangian objective as follows, for each variable $i$:
\begin{align*}
L_i &= \sum_{\phi \in \Phi} \mathrm{E}_{U_\phi \sim Q} [\ln \phi] + \mathrm{H}_Q(X_i) + \lambda_i (\sum_{x_i} Q(x_i) = 1) \\
&= \sum_{\phi \in \Phi} \sum_{u_\phi} Q(u_\phi) \ln \phi(u_\phi) + \mathrm{H}_Q(X_i) + \lambda_i (\sum_{x_i} Q(x_i) = 1)
\end{align*}
We take the derivative with respect to one variable $x_i$ now. (We need to do this for each variable in turn, and optimize with respect to each variable.)  Since
\begin{equation*}
\frac{\delta }{\delta Q(x_i)} \mathrm{E}_{U_\phi \sim Q} [\ln \phi] = \mathrm{E}_{U_\phi \sim Q} [\ln \phi \mid x_i]
\end{equation*}
we have the derivative of the lagrangian as
\begin{equation*}
\frac{\delta L_i }{\delta Q(x_i)} = \sum_{\phi \in \Phi} \mathrm{E}_{U_\phi \sim Q} [\ln \phi \mid x_i] - \ln Q(x_i) - 1 + \lambda_i
\end{equation*}
Setting this derivative equal to 0 and rearranging terms we get:
\begin{equation*}
\ln Q(x_i) = \lambda - 1 + \sum_{\phi \in \Phi} \mathrm{E}_{U_\phi \sim Q} [\ln \phi \mid x_i]
\end{equation*}
Continuing to follow Koller p.450-452, we exponentiate both sides and renormalize; because $\lambda_i$ is a constant relative to $x_i$, it drops out when we renormalize, so we get:
\begin{equation*}
Q(x_i) = \frac{1}{Z_i} \mathrm{exp} \left\lbrace \sum_{\phi \in \Phi} \mathrm{E}_{U_\phi \sim Q} [\ln \phi \mid x_i] \right\rbrace
\end{equation*}
which is the equation that must hold if and only if $Q(X_i)$ is a local maximum of the mean field given $\lbrace Q(X_j) \rbrace_{j \not = i}$.  We note that if $X_i \not \in Scope(\phi)$ then $\mathrm{E}_{U_\phi \sim Q} [\ln \phi \mid x_i] = \mathrm{E}_{U_\phi \sim Q} [\ln \phi]$.  Put another way, expectations on such factors do not depend on $x_i$, and may therefore be lumped into the normalization constant $Z_i$.  We can therefore simplify and get:
\begin{equation*}
Q(x_i) = \frac{1}{Z_i} \mathrm{exp} \left\lbrace \sum_{\phi : X_i \in Scope(\phi)} \mathrm{E}_{(U_\phi - \lbrace X_i \rbrace) \sim Q} [\ln \phi(U_\phi , x_i)] \right\rbrace
\end{equation*}
Given our network for $P$, we see that we have factors over $(A)$, $(BA)$, $(CAB)$, $(DB)$, $(ECD)$, and $(FD)$.  We see from our previous simplification that the update equation for $Q(x_i)$ only sums over each factor $\phi$ such that $X_i$ is in $\phi$'s scope, lumping everything else into the normalization constant.  If we define $f(\phi, x_i) = \mathrm{E}_{(U_\phi - \lbrace X_i \rbrace) \sim Q} [\ln \phi(U_\phi , x_i)]$, we get the following update equations:
\begin{align*}
Q(a) &= \frac{1}{Z_a} \mathrm{exp} \left\lbrace f(A,a) + f(BA,a) + f(CAB,a) \right\rbrace \\
Q(b) &= \frac{1}{Z_b} \mathrm{exp} \left\lbrace f(DB,b) + f(BA,b) + f(CAB,b) \right\rbrace \\
Q(c) &= \frac{1}{Z_c} \mathrm{exp} \left\lbrace f(CAB,c) + f(ECD,c) \right\rbrace \\
Q(d) &= \frac{1}{Z_d} \mathrm{exp} \left\lbrace f(ECD,d) + f(DB,d) + f(FD,d) \right\rbrace \\
Q(e) &= \frac{1}{Z_e} \mathrm{exp} \left\lbrace f(ECD,e) \right\rbrace \\
Q(f) &= \frac{1}{Z_f} \mathrm{exp} \left\lbrace f(FD,f) \right\rbrace \\
\end{align*}
We now look at $f(\phi, x_i)$.
\[
f(\phi, x_i) = \mathrm{E}_{(U_\phi - \lbrace X_i \rbrace) \sim Q} [\ln \phi(U_\phi , x_i)] = \sum_{u_\phi \in (U_\phi - \lbrace X_i \rbrace)} Q(u_\phi) \ln \phi(u_\phi, x_i)
\]
We note that all variables are binary.  We can also see that in $\phi(u_\phi, x_i)$ we are considering only the sub-factor that has $X_i = x_i$, and we are summing over all other values $u_\phi$ of the other variables $(U_\phi - \lbrace X_i \rbrace)$, and multiplying by $Q(u_\phi)$, which is a product of marginals.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{3 Analyzing Multiple Text Corpora}
\noindent {\bf(a) Changing Day 1 Ice creams from 2 to 1...}\\

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{4 Collapsed Gibbs Sampler Implementation}
\noindent {\bf(a) Changing Day 1 Ice creams from 2 to 1...}\\

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{5 Blocked Gibbs Sampler}
\noindent {\bf(5.1) Analysis Questions}\\
Given the full expanded form of the likelihood:\\
\begin{align}
P (z, x, c, w\mid \alpha, \beta, \lambda) &= P (w \mid z, x, c, \beta)P (z\mid \alpha)P(x\mid \lambda) \nonumber\\
&=\prod_{k}(\frac{\Gamma(\sum_{w} \beta)}{\Gamma(\sum_{w} n^{k}_{w}+\beta)} \prod_{w} \frac{\Gamma(n^{k}_{w}+\beta))}{\Gamma(\beta)} )\nonumber\\
&\times \prod_{c}\prod_{k}(\frac{\Gamma(\sum_{w} \beta)}{\Gamma(\sum_{w} n^{c,k}_{w}+\beta))} \prod_{w} \frac{\Gamma(n^{c,k}_{w}+\beta))}{\Gamma(\beta)} )\nonumber\\
&\times \prod_{d}(\frac{\Gamma(\sum_{k} \alpha)}{\Gamma(\sum_{k} n^{d}_{k}+\alpha)} \prod_{k} \frac{\Gamma(n^{d}_{k}+\alpha))}{\Gamma(\alpha)} ) \times \prod_{(d,i)} P(x_{d,i} \mid \lambda)\nonumber\\
\end{align}

\begin{align}
P (z_{d,i}, x_{d,i} \mid z-z_{d,i},x-x_{d,i},c,w;\alpha, \beta, \lambda) &= \frac{P (z, x, c, w\mid \alpha, \beta, \lambda)}{P (z-z_{d,i}, x-x_{d,i}, c, w\mid \alpha, \beta, \lambda)} \nonumber\\
\end{align}

\begin{align}
P &(z_{d,i}=k, x_{d,i} = 0 \mid z-z_{d,i},x-x_{d,i},c,w;\alpha, \beta, \lambda)  \nonumber\\
&=\frac{\frac{\Gamma(n^{k}_{w_{d,i}} +1+\beta)}{\Gamma(1+\sum_{w} n^{k}_{w}+\beta)}  \frac{\Gamma(n^{d}_{k} +1+\alpha)}{\Gamma(1+\sum_{k'} n^{k'}_{w}+\alpha)}}{\frac{\Gamma(n^{k}_{w_{d,i}} +\beta)}{\Gamma(\sum_{w} n^{k}_{w}+\beta)}  \frac{\Gamma(n^{d}_{k} +\alpha)}{\Gamma(\sum_{k'} n^{k'}_{w}+\alpha)}} P(x_{d,i} = 0 \mid \lambda)\nonumber\\
&= \frac{n^{k}_{w_{d,i}}+\beta}{n^{k}_{*}+V\beta}  \frac{n^{d}_{k} +\alpha}{n^{d}_{*} +K\alpha} (1-\lambda) 
\end{align}

\begin{align}
P &(z_{d,i}=k, x_{d,i} = 1 \mid z-z_{d,i},x-x_{d,i},c,w;\alpha, \beta, \lambda)  \nonumber\\
&=\frac{\frac{\Gamma(n^{c,k}_{w_{d,i}} +1+\beta)}{\Gamma(1+\sum_{w} n^{c,k}_{w}+\beta)}  \frac{\Gamma(n^{d}_{k} +1+\alpha)}{\Gamma(1+\sum_{k'} n^{k'}_{w}+\alpha)}}{\frac{\Gamma(n^{c,k}_{w_{d,i}} +\beta)}{\Gamma(\sum_{w} n^{c,k}_{w}+\beta)}  \frac{\Gamma(n^{d}_{k} +\alpha)}{\Gamma(\sum_{k'} n^{k'}_{w}+\alpha)}} P(x_{d,i} = 1 \mid \lambda)\nonumber\\
&= \frac{n^{c,k}_{w_{d,i}}+\beta}{n^{c,k}_{*}+V\beta}  \frac{n^{d}_{k} +\alpha}{n^{d}_{*} +K\alpha} (\lambda) 
\end{align}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{6 Text Analysis with MCLDA}
\noindent {\bf(1)} For each chain the training likelihood increases (monotonically, save for minor fluctuations)  till it plateaus around 300 iterations. Similarly the likelihood for test data decreases till it plateaus however with much more variation. \\
\\
\noindent {\bf(4)} TOPICS \\
\\
\noindent {\bf(5)} Lambdas\\
\\
\noindent {\bf(6)} Words \\
\noindent {\bf(a)} topics \\
\\
\noindent {\bf(b)} Varying $\lambda$ controls the amount we sample from global vs corpus dependent distribution of topics. Test performance increases with $\lambda$ initially but then peaks around .75 before decreasing again. This implies that there is an optimal lambda that is between always sampling from the global and always sampling from collection dependent. This intuitively makes sense because with $\lambda = 0$ this would imply that there is no advantage of looking at collection specific topics. The other extreme with $\lambda=1$ would imply that there was no benefit to generalizing and considering global topics. Everywhere in between implies that there are differences in which words each collection uses to describe a topic but there is still benefit to considering the generalized corpus. The fact that our value slightly prefers collection specific seems reasonable since means that documents mostly talk about collection specific topics (NLP vs ML) but global topics still contribute. 
 \\
\\
\noindent {\bf(a)} alpha/beta \\
\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{7 Variational Inference}
\noindent {\bf(7.1) Derivation}\\
We wish to derive the update equations for variational inference, as computing $p(\theta, c, \phi, z, x \mid w, \alpha, \beta, \lambda)$ for exact inference is intractable.  By definition of a conditional probability we have:
\begin{equation*}
\frac{p(\theta, c, \phi, z, x, w, \mid \alpha, \beta, \lambda)}{p(w \mid \alpha, \beta, \lambda)}
\end{equation*}
From the graphical model of our distribution at the top of page 6 of the homework PDF, it is easy to mutilate the graph of P to arrive at a more tractable family of approximate Q distributions.  We omit the $w$ node, and omit the edges between $\theta$, $z$, $\phi$, and $x$.  We also introduce free variational parameters $\gamma$, $\delta$, $\zeta$, and $\eta$ respectively on our factors of Q, themselves distributions.  $q(\theta \mid \gamma) \sim \mathrm{Dirichlet}(\gamma)$, $q(z \mid \delta) \sim \mathrm{Multinomial}(\delta)$, $q(\phi \mid \zeta) \sim \mathrm{Dirichlet}(\zeta)$, and $q(x \mid \eta) \sim \mathrm{Bernoulli}(\eta)$.\\
We note that we want to minimize the KL Divergence from $q$ to $p$, which is
\begin{equation*}
\mathrm{KL}(q\|p) = \ln(Z) - \mathrm{E}_q (\log p) - \mathrm{E}_q (\log q) 
\end{equation*}
and this is equivalent to maximizing the energy functional:
\begin{equation*}
F = \mathrm{E}_q (\log p) + \mathrm{E}_q (\log q)
\end{equation*}
We know that our joint of $p$ factorizes as the following:
\begin{equation*}
p(\theta, c, \phi, z, x, w, \mid \alpha, \beta, \lambda) = p(z \mid \theta) p(\theta \mid \alpha) p(x \mid \lambda) p(\phi \mid \beta) p(w \mid z, x, c, \phi)
\end{equation*}
and that our denominator in the conditional factorizes as the following:
\begin{equation*}
p(w, \mid \alpha, \beta, \lambda) = p(w, \mid \alpha, \beta, \lambda)
\end{equation*}
and that $p(w \mid z, x, c, \phi)$ factorizes as the following:
\begin{equation*}
p(w \mid z, x, c, \phi) = \prod_{(d,i)} ( p(w_{d,i} \mid \phi_k)^{I(x_{d,i} = 0)} p(w_{d,i} \mid \phi_{k}^{c})^{I(x_{d,i} = 1)} )
\end{equation*}
and that $q$ factorizes\ as the following:
\begin{equation*}
q(\theta, z, \phi, x \mid \gamma, \delta, \zeta, \eta) = q(\theta \mid \gamma) \prod_d q(z \mid \delta) q(\phi \mid \zeta) q(x \mid \eta)
\end{equation*}
so we can plug these into our definition of the energy functional.
\begin{align*}
F &= \mathrm{E}_q \{\log p(z \mid \theta) + \log p(\theta \mid \alpha) + \log p(x \mid \lambda) + \log p(\phi \mid \beta) \\
&+ \log p(w \mid z, x, c, \phi) - \log p(w, \mid \alpha, \beta, \lambda) \} \\
&+ \mathrm{E}_q \{ \log q(\theta \mid \gamma) + \sum_d \left( \log q(z \mid \delta) + \log q(\phi \mid \zeta) + \log q(x \mid \eta) \right) \} 
\end{align*}
Now, we can calculate the individual terms by taking the logarithm of each, also moving the expectation to each term, as the expectation of a sum is the sum of expectations.\\
\noindent \textit{Note that some of these equations were taken from\\ http://machinelearning.wustl.edu/mlpapers/paper\_files/BleiNJ03.pdf }
\begin{align*}
\mathrm{E}_q \log p(z \mid \theta) &= \sum\limits_{i=1}^{N_d} \sum\limits_{k=1}^K (\alpha - 1)(\Psi(\gamma_k) - \Psi(\sum_{j=1}^k \gamma_j) ) \\
\mathrm{E}_q\log p(\theta \mid \alpha) &= \log \Gamma (K \alpha) - \sum\limits_{k=1}^K \log \Gamma(\alpha) + \sum\limits_{k=1}^K (\alpha - 1)(\Psi(\gamma_k) - \Psi(\sum_{j=1}^K \gamma_j) )\\
\mathrm{E}_q\log p(x \mid \lambda) &= ??\\
\mathrm{E}_q\log p(\phi \mid \beta) &= \log \Gamma (K \beta) - \sum\limits_{k=1}^V \log \Gamma(\beta) + \sum\limits_{k=1}^V (\beta - 1)(\Psi(\zeta_k) - \Psi(\sum_{j=1}^V \zeta_j) )\\
\mathrm{E}_q\log p(w \mid z, x, c, \phi) &= \sum_{d,i} \left( I(x_{d,i} = 0) p(w_{d,i} \mid \phi_k) + I(x_{d,i} = 1) p(w_{d,i} \mid \phi_{k}^{c})  \right)\\
\mathrm{E}_q\log p(w, \mid \alpha, \beta, \lambda) &= ??\\
\mathrm{E}_q\log q(\theta \mid \gamma) &= \log \Gamma (\sum\limits_{k=1}^K \gamma_k) - \sum\limits_{k=1}^K \log \Gamma(\gamma_k) + \sum\limits_{k=1}^K (\gamma_k - 1)(\Psi(\gamma_k) - \Psi(\sum_{j=1}^K \gamma_j) )\\
\mathrm{E}_q\log q(z \mid \delta) &= \sum\limits_{i=1}^{N_d} \sum\limits_{k=1}^K \delta_{i,k} \log \delta_{i,k} \\
\mathrm{E}_q\log q(\phi \mid \zeta) &= \log \Gamma (\sum\limits_{k=1}^V \zeta_k) - \sum\limits_{k=1}^V \log \Gamma(\zeta_k) + \sum\limits_{k=1}^V (\zeta_k - 1)(\Psi(\zeta_k) - \Psi(\sum_{j=1}^V \zeta_j) )\\
\mathrm{E}_q\log q(x \mid \eta) &= ??\\
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
