%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%This is a science homework template. Modify the preamble to suit your needs. 
%The junk text is   there for you to immediately see how the headers/footers look at first 
%typesetting.


\documentclass[12pt]{article}

%AMS-TeX packages
\usepackage{amssymb,amsmath,amsthm} 
%geometry (sets margin) and other useful packages
\usepackage[margin=1.25in]{geometry}
\usepackage{graphicx,placeins}


%
%Redefining sections as problems
%
\makeatletter
\newenvironment{problem}{\@startsection
       {section}
       {1}
       {-.2em}
       {-3.5ex plus -1ex minus -.2ex}
       {2.3ex plus .2ex}
       {\pagebreak[3]%forces pagebreak when space is small; use \eject for better results
       \large\bf\noindent{Problem }
       }
       }
       {%\vspace{1ex}\begin{center} \rule{0.3\linewidth}{.3pt}\end{center}}
       \begin{center}\large\bf \ldots\ldots\ldots\end{center}}
\makeatother


%
%Fancy-header package to modify header/page numbering 
%
\usepackage{fancyhdr}
\pagestyle{fancy}
%\addtolength{\headwidth}{\marginparsep} %these change header-rule width
%\addtolength{\headwidth}{\marginparwidth}
\lhead{Problem \thesection}
\chead{} 
\rhead{\thepage} 
\lfoot{\small\scshape Machine Learning in Complex Domains} 
\cfoot{} 
\rfoot{\footnotesize PS \#3} 
\renewcommand{\headrulewidth}{.3pt} 
\renewcommand{\footrulewidth}{.3pt}
\setlength\voffset{-0.25in}
\setlength\textheight{648pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%
%Contents of problem set
%    
\begin{document}

\title{MLCD 3: Posterior Inference}
\author{Mike Smith and Elan Hourticolon-Retzler}

\maketitle

\thispagestyle{empty}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{2 Variational Inference}
\noindent {\bf(a) Changing Day 1 Ice creams from 2 to 1...}\\

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{3 Analyzing Multiple Text Corpora}
\noindent {\bf(a) Changing Day 1 Ice creams from 2 to 1...}\\

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{4 Collapsed Gibbs Sampler Implementation}
\noindent {\bf(a) Changing Day 1 Ice creams from 2 to 1...}\\

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{5 Blocked Gibbs Sampler}
\noindent {\bf(5.1) Analysis Questions}\\
Given the full expanded form of the likelihood:\\
\begin{align}
P (z, x, c, w\mid \alpha, \beta, \lambda) &= P (w \mid z, x, c, \beta)P (z\mid \alpha)P(x\mid \lambda) \nonumber\\
&=\prod_{k}(\frac{\Gamma(\sum_{w} \beta)}{\Gamma(\sum_{w} n^{k}_{w}+\beta)} \prod_{w} \frac{\Gamma(n^{k}_{w}+\beta))}{\Gamma(\beta)} )\nonumber\\
&\times \prod_{c}\prod_{k}(\frac{\Gamma(\sum_{w} \beta)}{\Gamma(\sum_{w} n^{c,k}_{w}+\beta))} \prod_{w} \frac{\Gamma(n^{c,k}_{w}+\beta))}{\Gamma(\beta)} )\nonumber\\
&\times \prod_{d}(\frac{\Gamma(\sum_{k} \alpha)}{\Gamma(\sum_{k} n^{d}_{k}+\alpha)} \prod_{k} \frac{\Gamma(n^{d}_{k}+\alpha))}{\Gamma(\alpha)} ) \times \prod_{(d,i)} P(x_{d,i} \mid \lambda)\nonumber\\
\end{align}

\begin{align}
P (z_{d,i}, x_{d,i} \mid z-z_{d,i},x-x_{d,i},c,w;\alpha, \beta, \lambda) &= \frac{P (z, x, c, w\mid \alpha, \beta, \lambda)}{P (z-z_{d,i}, x-x_{d,i}, c, w\mid \alpha, \beta, \lambda)} \nonumber\\
\end{align}

\begin{align}
P &(z_{d,i}=k, x_{d,i} = 0 \mid z-z_{d,i},x-x_{d,i},c,w;\alpha, \beta, \lambda)  \nonumber\\
&=\frac{\frac{\Gamma(n^{k}_{w_{d,i}} +1+\beta)}{\Gamma(1+\sum_{w} n^{k}_{w}+\beta)}  \frac{\Gamma(n^{d}_{k} +1+\alpha)}{\Gamma(1+\sum_{k'} n^{k'}_{w}+\alpha)}}{\frac{\Gamma(n^{k}_{w_{d,i}} +\beta)}{\Gamma(\sum_{w} n^{k}_{w}+\beta)}  \frac{\Gamma(n^{d}_{k} +\alpha)}{\Gamma(\sum_{k'} n^{k'}_{w}+\alpha)}} P(x_{d,i} = 0 \mid \lambda)\nonumber\\
&= \frac{n^{k}_{w_{d,i}}+\beta}{n^{k}_{*}+V\beta}  \frac{n^{d}_{k} +\alpha}{n^{d}_{*} +K\alpha} (1-\lambda) 
\end{align}

\begin{align}
P &(z_{d,i}=k, x_{d,i} = 1 \mid z-z_{d,i},x-x_{d,i},c,w;\alpha, \beta, \lambda)  \nonumber\\
&=\frac{\frac{\Gamma(n^{c,k}_{w_{d,i}} +1+\beta)}{\Gamma(1+\sum_{w} n^{c,k}_{w}+\beta)}  \frac{\Gamma(n^{d}_{k} +1+\alpha)}{\Gamma(1+\sum_{k'} n^{k'}_{w}+\alpha)}}{\frac{\Gamma(n^{c,k}_{w_{d,i}} +\beta)}{\Gamma(\sum_{w} n^{c,k}_{w}+\beta)}  \frac{\Gamma(n^{d}_{k} +\alpha)}{\Gamma(\sum_{k'} n^{k'}_{w}+\alpha)}} P(x_{d,i} = 1 \mid \lambda)\nonumber\\
&= \frac{n^{c,k}_{w_{d,i}}+\beta}{n^{c,k}_{*}+V\beta}  \frac{n^{d}_{k} +\alpha}{n^{d}_{*} +K\alpha} (\lambda) 
\end{align}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{6 Text Analysis with MCLDA}
\noindent {\bf(a) Changing Day 1 Ice creams from 2 to 1...}\\

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{7 Variational Inference}
\noindent {\bf(7.1) Derivation}\\
We wish to derive the update equations for variational inference, as computing $p(\theta, c, \phi, z, x \mid w, \alpha, \beta, \lambda)$ for exact inference is intractable.  By definition of a conditional probability we have:
\begin{equation*}
\frac{p(\theta, c, \phi, z, x, w, \mid \alpha, \beta, \lambda)}{p(w \mid \alpha, \beta, \lambda)}
\end{equation*}
From the graphical model of our distribution at the top of page 6 of the homework PDF, it is easy to mutilate the graph of P to arrive at a more tractable family of approximate Q distributions.  We omit the $w$ node, and omit the edges between $\theta$, $z$, $\phi$, and $x$.  We also introduce free variational parameters $\gamma$, $\delta$, $\zeta$, and $\eta$ respectively on our factors of Q, themselves distributions.  $q(\theta \mid \gamma) \sim \mathrm{Dirichlet}(\gamma)$, $q(z \mid \delta) \sim \mathrm{Multinomial}(\delta)$, $q(\phi \mid \zeta) \sim \mathrm{Dirichlet}(\zeta)$, and $q(x \mid \eta) \sim \mathrm{Bernoulli}(\eta)$.\\
We note that we want to minimize the KL Divergence from $q$ to $p$, which is
\begin{equation*}
\mathrm{KL}(q\|p) = \ln(Z) - \mathrm{E}_q (\log p) - \mathrm{E}_q (\log q) 
\end{equation*}
and this is equivalent to maximizing the energy functional:
\begin{equation*}
F = \mathrm{E}_q (\log p) + \mathrm{E}_q (\log q)
\end{equation*}
We know that our joint of $p$ factorizes as the following:
\begin{equation*}
p(\theta, c, \phi, z, x, w, \mid \alpha, \beta, \lambda) = p(z \mid \theta) p(\theta \mid \alpha) p(x \mid \lambda) p(\phi \mid \beta) p(w \mid z, x, c, \phi)
\end{equation*}
and that our denominator in the conditional factorizes as the following:
\begin{equation*}
p(w, \mid \alpha, \beta, \lambda) = p(w, \mid \alpha, \beta, \lambda)
\end{equation*}
and that $p(w \mid z, x, c, \phi)$ factorizes as the following:
\begin{equation*}
p(w \mid z, x, c, \phi) = \prod_{(d,i)} ( p(w_{d,i} \mid \phi_k)^{I(x_{d,i} = 0)} p(w_{d,i} \mid \phi_{k}^{c})^{I(x_{d,i} = 1)} )
\end{equation*}
and that $q$ factorizes\ as the following:
\begin{equation*}
q(\theta, z, \phi, x \mid \gamma, \delta, \zeta, \eta) = q(\theta \mid \gamma) \prod_d q(z \mid \delta) q(\phi \mid \zeta) q(x \mid \eta)
\end{equation*}
so we can plug these into our definition of the energy functional.
\begin{align*}
F &= \mathrm{E}_q \{\log p(z \mid \theta) + \log p(\theta \mid \alpha) + \log p(x \mid \lambda) + \log p(\phi \mid \beta) \\
&+ \log p(w \mid z, x, c, \phi) - \log p(w, \mid \alpha, \beta, \lambda) \} \\
&+ \mathrm{E}_q \{ \log q(\theta \mid \gamma) + \sum_d \left( \log q(z \mid \delta) + \log q(\phi \mid \zeta) + \log q(x \mid \eta) \right) \} 
\end{align*}
Now, we can calculate the individual terms by taking the logarithm of each, also moving the expectation to each term, as the expectation of a sum is the sum of expectations.\\
\noindent \textit{Note that some of these equations were taken from\\ http://machinelearning.wustl.edu/mlpapers/paper\_files/BleiNJ03.pdf }
\begin{align*}
\mathrm{E}_q \log p(z \mid \theta) &= \sum\limits_{i=1}^{N_d} \sum\limits_{k=1}^K (\alpha - 1)(\Psi(\gamma_k) - \Psi(\sum_{j=1}^k \gamma_j) ) \\
\mathrm{E}_q\log p(\theta \mid \alpha) &= \log \Gamma (K \alpha) - \sum\limits_{k=1}^K \log \Gamma(\alpha) + \sum\limits_{k=1}^K (\alpha - 1)(\Psi(\gamma_k) - \Psi(\sum_{j=1}^K \gamma_j) )\\
\mathrm{E}_q\log p(x \mid \lambda) &= ??\\
\mathrm{E}_q\log p(\phi \mid \beta) &= \log \Gamma (K \beta) - \sum\limits_{k=1}^V \log \Gamma(\beta) + \sum\limits_{k=1}^V (\beta - 1)(\Psi(\zeta_k) - \Psi(\sum_{j=1}^V \zeta_j) )\\
\mathrm{E}_q\log p(w \mid z, x, c, \phi) &= \sum_{d,i} \left( I(x_{d,i} = 0) p(w_{d,i} \mid \phi_k) + I(x_{d,i} = 1) p(w_{d,i} \mid \phi_{k}^{c})  \right)\\
\mathrm{E}_q\log p(w, \mid \alpha, \beta, \lambda) &= ??\\
\mathrm{E}_q\log q(\theta \mid \gamma) &= \log \Gamma (\sum\limits_{k=1}^K \gamma_k) - \sum\limits_{k=1}^K \log \Gamma(\gamma_k) + \sum\limits_{k=1}^K (\gamma_k - 1)(\Psi(\gamma_k) - \Psi(\sum_{j=1}^K \gamma_j) )\\
\mathrm{E}_q\log q(z \mid \delta) &= \sum\limits_{i=1}^{N_d} \sum\limits_{k=1}^K \delta_{i,k} \log \delta_{i,k} \\
\mathrm{E}_q\log q(\phi \mid \zeta) &= \log \Gamma (\sum\limits_{k=1}^V \zeta_k) - \sum\limits_{k=1}^V \log \Gamma(\zeta_k) + \sum\limits_{k=1}^V (\zeta_k - 1)(\Psi(\zeta_k) - \Psi(\sum_{j=1}^V \zeta_j) )\\
\mathrm{E}_q\log q(x \mid \eta) &= ??\\
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
